# Project Baarat: Empowering Regional Languages in India 🇮🇳 with AI


<div align="center">
   
   # Project Baarat 🚀
   
   ![Project Baarat](https://github.com/asphytheghoul/Baarat/assets/52605103/8c1ba4c4-03e6-4067-9a8e-fb65b7d8a2e0)
</div>

Project Baarat is an open-source initiative to leverage the power of
LLMs on Indic-NLP tasks. We aim to build Continually pre-trained, Task
Specific Language Models in a Mixture of Experts (MoE) setup through
domain adaptive pre-training. We plan on making a **multilingual**
**and**  **cross-lingual** LLM that is :

  

> 1\) Pre-trained on a large text corpus containing various sources of
> knowledge including crawled wikipedia articles, textbooks, news,
> social media sites, magazines etc.

>

> 2\) Is continually pre-trained on different downstream tasks. We first
> train a 7B LLaMa-2 model on an unsupervised text corpus in the target
> language and save it as a base model. We have considered the following
> tasks as downstream tasks that will be incorporated in the fine-tuning
> process:

>

> ● Machine Translation 
> ● Text Summarization 
> ● Question Answering 
> ● Instruct Fine-Tuning

>

> (this list is subject to change and a few tasks may be added over time).
  
## About Project Baarat 📖

Project Baarat is dedicated to making indigenous (regional) languages more accessible. With a focus on the rich linguistic diversity of India. This project aims to break language barriers and promote inclusivity through technology.
<br/>
<br/>

### Key Features ✨

- **Tokenizers for Indian Languages**: Robust tokenization tools tailored for the unique structures of regional Indian languages.
- **Fine-tuned Language Models**: Leveraging the power of Large Language Models (LLMs) fine-tuned for Indian languages to understand and generate text with high accuracy.
- **Open Source Collaboration**: We believe in the collective power of the community to drive innovation and inclusivity. 🤝
<br/>
<br/>

## Our Vision 🌟

To promote the spirit of building accessible models in native languages, fostering a world where technology speaks everyone's language. 🗣️
<br/>
<br/>

## Roadmap 🛣️

- ✅ ~~Prepare and setup dataset~~
- ✅ ~~Prepare and setup tokenizers~~
- ✅ ~~Start pre-training~~
- ✅ ~~Fine-tune models~~
- ⬜ Implement gating mechanism


Foundational model: LLaMa-2 7B
<br/>



## Contribute to Project Baarat 🛠️

We welcome open-source contributions! Whether you're a coder, a linguist, or just someone passionate about language accessibility, there's a place for you in Project Baarat. Here's how you can get involved:

1. **Star and Fork**: Give us a star ⭐ on GitHub and fork the repository to start contributing.
2. **Issue Tracker**: Report bugs or suggest new features by creating an issue.
3. **Pull Requests**: Submit your pull requests with new features, bug fixes, or documentation enhancements.

Check out our [CONTRIBUTING.md](./CONTRIBUTING.md) for more detailed guidelines.
<br/>
<br/>

## License 📄

Project Baarat is released under the [MIT License](./LICENSE).
<br/>
<br/>

## Show Your Support 🌈

If you like Project Baarat, please consider starring the repository and sharing it with your network!
<br/>
<br/>

---

Made with ❤️ by Team Baarat,\
  [Akash Kamalesh](https://github.com/asphytheghoul) , [Anirudh Lakhotia](https://github.com/anirudhlakhotia/) and [Tanistha Hota](https://github.com/hota15), PES University, Bengaluru.



